{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Requirement Install\n"
      ],
      "metadata": {
        "id": "FxAdDVtitpV_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bskO8Q_etG1n",
        "outputId": "2129ab23-47c1-4a77-f968-436cb538e11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn\n",
        "!pip install konlpy\n",
        "!pip install torch             # 또는 GPU:\n",
        "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "Fxw4mVv-tmO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 음식점 리뷰 키워드 추출\n",
        "\n",
        "### Output File Name :\n",
        "Sample_음식점/음식점_all_reviews_with_keywords_sentiment_all.csv"
      ],
      "metadata": {
        "id": "B2URUhezvaSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # BLAS 스레드 제한\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1) 데이터 로드 및 컬럼 정리\n",
        "file_paths = glob('/content/drive/MyDrive/Sample_음식점/음식점_리뷰크롤링_ALL.csv')\n",
        "\n",
        "df = pd.concat([\n",
        "    pd.read_csv(fp, encoding='utf-8', engine='python')\n",
        "    for fp in file_paths\n",
        "], ignore_index=True)\n",
        "\n",
        "# 2) 리뷰 컬럼이름 통일 & 빈 값 제거\n",
        "if '리뷰내용' not in df.columns:\n",
        "    raise ValueError(\"컬럼 '리뷰내용'이 없습니다.\")\n",
        "df = df[['리뷰내용']].dropna(subset=['리뷰내용'])\n",
        "df = df.rename(columns={'리뷰내용':'review'})\n",
        "df['review'] = df['review'].astype(str)\n",
        "# 2) 불용어 사전\n",
        "STOPWORDS = set(['이','가','은','는','도','에','와','과','로','으로','의','를','을','하다'])\n",
        "\n",
        "# 3) 형태소 분석기\n",
        "okt = Okt()\n",
        "def preprocess(text: str) -> str:\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # 특수문자 제거\n",
        "    text = re.sub(r'\\d+', ' ', text)      # 숫자 제거\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    pos = okt.pos(text, stem=True)\n",
        "    return [\n",
        "        word for word, tag in pos\n",
        "        if tag in ['Noun','Adjective','Verb'] and word not in STOPWORDS\n",
        "    ]\n",
        "\n",
        "# 4) 키워드 추출 함수 (SBERT)\n",
        "KW_MODEL = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "def extract_keywords(doc: str, top_k: int = 5) -> list[str]:\n",
        "    # 전처리 & 토크나이즈\n",
        "    clean = preprocess(doc)\n",
        "    toks  = tokenize(clean)\n",
        "    if not toks:\n",
        "        return []\n",
        "\n",
        "    # TF-IDF 로 후보 만들기 (에러 방어)\n",
        "    try:\n",
        "        tfidf = TfidfVectorizer(ngram_range=(1,1)).fit([' '.join(toks)])\n",
        "        candidates = tfidf.get_feature_names_out()\n",
        "    except ValueError:\n",
        "        return []\n",
        "\n",
        "    if len(candidates) == 0:\n",
        "        return []\n",
        "\n",
        "    # 실제 뽑을 키워드 개수\n",
        "    k = min(top_k, len(candidates))\n",
        "\n",
        "    # 문장과 후보 임베딩\n",
        "    emb_doc = KW_MODEL.encode(doc, convert_to_tensor=True)\n",
        "    emb_kw  = KW_MODEL.encode(candidates, convert_to_tensor=True)\n",
        "\n",
        "    # 코사인 유사도 상위 k개\n",
        "    scores = util.pytorch_cos_sim(emb_doc, emb_kw)[0]\n",
        "    topk_idx = scores.topk(k).indices.cpu().numpy()\n",
        "    return [candidates[i] for i in topk_idx]\n",
        "\n",
        "# 5) 감성분석 파이프라인\n",
        "sentiment = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model='monologg/koelectra-base-v3-discriminator',\n",
        "    tokenizer='monologg/koelectra-base-v3-discriminator',\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "# 6) DataFrame에 적용\n",
        "df['cleaned']   = df['review'].map(preprocess)\n",
        "df['tokens']    = df['cleaned'].map(tokenize)\n",
        "df['keywords']  = df['review'].map(lambda x: extract_keywords(x, top_k=5))\n",
        "df['sentiment'] = df['review'].map(lambda x: sentiment(x)[0]['label'])\n",
        "\n",
        "# 7) 결과 확인\n",
        "print(df[['review','keywords','sentiment']].head())\n",
        "\n",
        "# 8) CSV로 저장\n",
        "df.to_csv('/content/drive/MyDrive/Sample_음식점/음식점_all_reviews_with_keywords_sentiment_all.csv', index=False)\n"
      ],
      "metadata": {
        "id": "t2zXLJrJvZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 숙박 리뷰 키워드 추출\n",
        "\n",
        "Output File Name :\n",
        "Sample/Store Name + 숙박_all_reviews_with_keywords_sentiment.csv\n"
      ],
      "metadata": {
        "id": "8QwabmrVv8gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # BLAS 스레드 제한\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1) 데이터 로드 및 컬럼 정리\n",
        "file_paths = glob('/content/drive/MyDrive/Sample/네이버 지도 방문자 리뷰 크롤러_숙박_*.csv')\n",
        "\n",
        "df = pd.concat([\n",
        "    pd.read_csv(fp,\n",
        "                usecols=['리뷰내용','가게이름','카테고리','전체평점','방문자리뷰','리뷰작성자','이런_점이_좋아요','방문시간'],\n",
        "                encoding='utf-8',    # 필요에 따라 'cp949' 등으로 변경\n",
        "                engine='python')\n",
        "    for fp in file_paths\n",
        "], ignore_index=True)\n",
        "\n",
        "# 2) 컬럼명 통일 및 결측치 제거\n",
        "#   - 리뷰가 없는 행은 제거하지만, 가게명은 그대로 보존합니다.\n",
        "df = df.dropna(subset=['리뷰내용'])\n",
        "df = df.rename(columns={\n",
        "    '리뷰내용': 'review'\n",
        "})\n",
        "df['review'] = df['review'].astype(str)\n",
        "\n",
        "# 2) 불용어 사전\n",
        "STOPWORDS = set(['이','가','은','는','도','에','와','과','로','으로','의','를','을','하다'])\n",
        "\n",
        "# 3) 형태소 분석기\n",
        "okt = Okt()\n",
        "def preprocess(text: str) -> str:\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # 특수문자 제거\n",
        "    text = re.sub(r'\\d+', ' ', text)      # 숫자 제거\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    pos = okt.pos(text, stem=True)\n",
        "    return [\n",
        "        word for word, tag in pos\n",
        "        if tag in ['Noun','Adjective','Verb'] and word not in STOPWORDS\n",
        "    ]\n",
        "\n",
        "# 4) 키워드 추출 함수 (SBERT)\n",
        "KW_MODEL = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "def extract_keywords(doc: str, top_k: int = 5) -> list[str]:\n",
        "    # 전처리 & 토크나이즈\n",
        "    clean = preprocess(doc)\n",
        "    toks  = tokenize(clean)\n",
        "    if not toks:\n",
        "        return []\n",
        "\n",
        "    # TF-IDF 로 후보 만들기 (에러 방어)\n",
        "    try:\n",
        "        tfidf = TfidfVectorizer(ngram_range=(1,1)).fit([' '.join(toks)])\n",
        "        candidates = tfidf.get_feature_names_out()\n",
        "    except ValueError:\n",
        "        return []\n",
        "\n",
        "    if len(candidates) == 0:\n",
        "        return []\n",
        "\n",
        "    # 실제 뽑을 키워드 개수\n",
        "    k = min(top_k, len(candidates))\n",
        "\n",
        "    # 문장과 후보 임베딩\n",
        "    emb_doc = KW_MODEL.encode(doc, convert_to_tensor=True)\n",
        "    emb_kw  = KW_MODEL.encode(candidates, convert_to_tensor=True)\n",
        "\n",
        "    # 코사인 유사도 상위 k개\n",
        "    scores = util.pytorch_cos_sim(emb_doc, emb_kw)[0]\n",
        "    topk_idx = scores.topk(k).indices.cpu().numpy()\n",
        "    return [candidates[i] for i in topk_idx]\n",
        "\n",
        "# 5) 감성분석 파이프라인\n",
        "sentiment = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model='monologg/koelectra-base-v3-discriminator',\n",
        "    tokenizer='monologg/koelectra-base-v3-discriminator',\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "# 6) DataFrame에 적용\n",
        "df['cleaned']   = df['review'].map(preprocess)\n",
        "df['tokens']    = df['cleaned'].map(tokenize)\n",
        "df['keywords']  = df['review'].map(lambda x: extract_keywords(x, top_k=5))\n",
        "df['sentiment'] = df['review'].map(lambda x: sentiment(x)[0]['label'])\n",
        "\n",
        "# 7) 결과 확인\n",
        "print(df[['가게이름','카테고리','전체평점','방문자리뷰','리뷰작성자','이런_점이_좋아요','방문시간','review','keywords','sentiment']].head())\n",
        "\n",
        "# 8) CSV로 저장\n",
        "df.to_csv('/content/drive/MyDrive/Sample/Store Name + 숙박_all_reviews_with_keywords_sentiment.csv', index=False)\n"
      ],
      "metadata": {
        "id": "yhCjJl9zwIKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 상위 5개 키워드 추출"
      ],
      "metadata": {
        "id": "Z4fg6Ns6thgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# 리뷰 데이터 (예시)\n",
        "# df에는 최소 다음 컬럼들이 있어야 합니다: '가게이름', 'keywords'\n",
        "# 'keywords'는 각 리뷰에 대해 추출된 키워드 리스트 형태 (예: ['조용하다', '깨끗하다', '가깝다'])\n",
        "\n",
        "# 1. 결측치 제거 및 확인\n",
        "df_keywords = data.dropna(subset=['가게이름', 'keywords'])\n",
        "df_keywords['keywords'] = df_keywords['keywords'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# 2. 장소별 키워드 누적\n",
        "store_keywords = df_keywords.groupby('가게이름')['keywords'].sum()\n",
        "\n",
        "# 3. 각 장소에서 상위 5개 키워드 추출\n",
        "top_keywords_per_store = store_keywords.apply(lambda x: [kw for kw, _ in Counter(x).most_common(8)])\n",
        "\n",
        "# 4. DataFrame으로 변환\n",
        "top_keywords_df = top_keywords_per_store.reset_index()\n",
        "top_keywords_df.columns = ['가게이름', '상위_키워드']\n",
        "top_keywords_df.to_csv('/content/drive/MyDrive/Sample/25_05_10_Store Name + 숙박_all_reviews_with_keywords_sentiment.csv', index=False)\n",
        "\n",
        "# 결과 확인\n",
        "print(top_keywords_df.head(50))\n"
      ],
      "metadata": {
        "id": "lHVntdvNtgwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##관광지 키워드 이용 추천 시스템\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8bUgqiZ7yDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "\n",
        "# 1) 데이터 불러오기 -------------------------------------------------------\n",
        "\n",
        "meta_df = pd.read_csv('/content/drive/MyDrive/Meta Data/jeju_tour_spot.csv',\n",
        "    usecols=[\n",
        "      'contents_id','contents_label','title','address','road_address','tag',\n",
        "      'introduction','latitude','longitude',\n",
        "      '평일오픈시간','평일클로즈시간','주말오픈시간','주말클로즈시간'\n",
        "    ]\n",
        ")\n",
        "reviews_df = pd.read_csv('/content/drive/MyDrive/Sample_관광지/Store Name + 관광지_all_reviews_with_quarter.csv',\n",
        "    usecols=[\n",
        "      '가게이름','카테고리','전체평점','방문자리뷰',\n",
        "      '리뷰작성자','review','이런_점이_좋아요','방문시간',\n",
        "      'cleaned','tokens','keywords','sentiment','quarter'\n",
        "    ]\n",
        ")\n",
        "\n",
        "meta_map = meta_df[['contents_id','title']].rename(columns={'title':'가게이름'})\n",
        "reviews_df = reviews_df.merge(meta_map, on='가게이름', how='left')\n",
        "\n",
        "# 2) 장소별 키워드 집계 -------------------------------------------------------\n",
        "def aggregate_top_keywords(df, place_col='contents_id', kw_col='keywords', top_n=5):\n",
        "    # 리뷰별 keywords 리스트 explode\n",
        "    exploded = df[[place_col, kw_col]].explode(kw_col).dropna(subset=[kw_col])\n",
        "    counts = (\n",
        "      exploded\n",
        "      .groupby([place_col, kw_col])\n",
        "      .size()\n",
        "      .reset_index(name='cnt')\n",
        "    )\n",
        "    # 장소별 상위 top_n 키워드만 추출\n",
        "    topk = (\n",
        "      counts\n",
        "      .sort_values([place_col,'cnt'], ascending=[True,False])\n",
        "      .groupby(place_col)\n",
        "      .head(top_n)\n",
        "      .groupby(place_col)[kw_col]\n",
        "      .apply(list)\n",
        "      .reset_index()\n",
        "      .rename(columns={kw_col:'top_keywords'})\n",
        "    )\n",
        "    # 리스트를 공백으로 합쳐서 한 문장으로\n",
        "    topk['keyword_text'] = topk['top_keywords'].map(lambda kws: \" \".join(kws))\n",
        "    return topk[[place_col,'keyword_text']]\n",
        "\n",
        "store_kw = aggregate_top_keywords(reviews_df, place_col='contents_id', kw_col='keywords', top_n=5)\n",
        "meta_df = meta_df.merge(store_kw, on='contents_id', how='left').fillna({'keyword_text':''})\n",
        "\n",
        "\n",
        "# 3) SBERT 임베딩 & 유사도 ----------------------------------------------------\n",
        "sbert = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "# 장소별 키워드 문장 임베딩 (tensor list)\n",
        "meta_df['kw_emb'] = list(\n",
        "    sbert.encode(\n",
        "      meta_df['keyword_text'].tolist(),\n",
        "      convert_to_tensor=True\n",
        "    )\n",
        ")\n",
        "# 1) 모든 임베딩을 (N, D) 형태로 쌓기\n",
        "embs = torch.stack(meta_df['kw_emb'].tolist(), dim=0)\n",
        "\n",
        "# 2) N×N 코사인 유사도 행렬 계산\n",
        "sim_matrix = util.pytorch_cos_sim(embs, embs).cpu().numpy()\n",
        "\n",
        "\n",
        "# 4) Haversine 거리 계산 -----------------------------------------------------\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0  # 지구 반경 (km)\n",
        "    dlat = radians(lat2 - lat1)\n",
        "    dlon = radians(lon2 - lon1)\n",
        "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
        "    return R * 2 * atan2(sqrt(a), sqrt(1-a))\n",
        "\n",
        "coords = meta_df[['latitude','longitude']].to_numpy()\n",
        "n = len(coords)\n",
        "dist_matrix = np.zeros((n,n), dtype=float)\n",
        "for i in range(n):\n",
        "    for j in range(i+1, n):\n",
        "        d = haversine(*coords[i], *coords[j])\n",
        "        dist_matrix[i,j] = dist_matrix[j,i] = d\n",
        "\n",
        "\n",
        "# 5) Co-Review Count 생성 ----------------------------------------------------\n",
        "# 같은 (year,quarter,user_id)에 리뷰를 남긴 장소 쌍 수를 센다.\n",
        "pair_counter = Counter()\n",
        "grp = (\n",
        "    reviews_df\n",
        "    .dropna(subset=['contents_id','review'])\n",
        "    .groupby(['quarter','리뷰작성자'])\n",
        "    ['contents_id']\n",
        "    .apply(lambda lst: sorted(set(lst)))\n",
        ")\n",
        "for places in tqdm(grp, desc=\"building co-review\"):\n",
        "    for i, j in combinations(places, 2):\n",
        "        pair_counter[(i,j)] += 1\n",
        "\n",
        "co_pairs = [\n",
        "    {'place1': i, 'place2': j, 'co_count': cnt}\n",
        "    for (i,j), cnt in pair_counter.items()\n",
        "]\n",
        "co_df = pd.DataFrame(co_pairs)\n",
        "\n",
        "\n",
        "# 6) 피처 테이블 생성 --------------------------------------------------------\n",
        "# place_id → meta_df 인덱스 매핑\n",
        "place2idx = {pid: idx for idx, pid in enumerate(meta_df['contents_id'])}\n",
        "\n",
        "def make_feature_table(co_df, meta_df):\n",
        "    rows = []\n",
        "    for _, r in tqdm(co_df.iterrows(), total=len(co_df), desc=\"feature table\"):\n",
        "        i, j = r['place1'], r['place2']\n",
        "        if i not in place2idx or j not in place2idx:\n",
        "            continue\n",
        "        idx_i, idx_j = place2idx[i], place2idx[j]\n",
        "        rows.append({\n",
        "            'place1': i,\n",
        "            'place2': j,\n",
        "            'co_count': r['co_count'],\n",
        "            'dist_km':    dist_matrix[idx_i, idx_j],\n",
        "            'sbert_sim':  sim_matrix[idx_i, idx_j],\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "feat_df = make_feature_table(co_df, meta_df)\n",
        "\n",
        "\n",
        "# 7) LightGBM 학습 파이프라인 예시 ------------------------------------------\n",
        "# 회귀(regression) 혹은 분류(classification) 태스크로 바꾸셔도 됩니다.\n",
        "X = feat_df.drop(columns=['place1','place2','co_count'])\n",
        "y = feat_df['co_count']\n",
        "\n",
        "lgb_train = lgb.Dataset(X, y)\n",
        "params = {\n",
        "    'objective':'regression',\n",
        "    'metric':'rmse',\n",
        "    'learning_rate':0.1,\n",
        "}\n",
        "gbm = lgb.train(params, lgb_train, num_boost_round=200)\n",
        "\n",
        "\n",
        "# 8) 결과 저장 ---------------------------------------------------------------\n",
        "# 메타: 키워드 문장만 남기고 embedding은 삭제\n",
        "meta_df.drop(columns=['kw_emb'], inplace=True)\n",
        "meta_df.to_csv('meta_with_kw.csv', index=False)\n",
        "feat_df.to_csv('item_cf_features.csv', index=False)\n",
        "\n",
        "print(\"파이프라인 완료\")"
      ],
      "metadata": {
        "id": "yf6e9LHu8J58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import pandas as pd\n",
        "meta_df = pd.read_csv('/content/drive/MyDrive/Meta Data/jeju_tour_spot.csv',\n",
        "    usecols=[\n",
        "        'contents_id','title',\n",
        "        'tag','introduction'\n",
        "    ]\n",
        ")\n",
        "\n",
        "#    키워드 텍스트 불러오기\n",
        "kw_df = pd.read_csv('/content/drive/MyDrive/Sample/25_05_10_Store Name + 숙박_all_reviews_with_keywords_sentiment.csv',\n",
        "    usecols=['가게이름','상위_키워드']\n",
        ")\n",
        "\n",
        "kw_df = kw_df.rename(columns={\n",
        "    '가게이름':     'title',\n",
        "    '상위_키워드': 'keyword_text'\n",
        "})\n",
        "# 병합 (LEFT JOIN)\n",
        "df = meta_df.merge(kw_df, on='title', how='left')\n",
        "\n",
        "# 누락된 키워드는 빈 문자열로\n",
        "df['keyword_text'] = df['keyword_text'].fillna('')\n",
        "\n",
        "# tag와 keyword_text 합쳐서 추천용 텍스트 생성\n",
        "df['combined_text'] = (\n",
        "    df['tag'].fillna('') + ' ' + df['keyword_text']\n",
        ").str.strip()\n",
        "\n",
        "\n",
        "# 1) SBERT 모델 로드\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 2) 사용자가 입력한 키워드 리스트 예시\n",
        "user_kw = ['산책', '해안도로','드라이브']\n",
        "user_sent = \" \".join(user_kw)\n",
        "emb_user = model.encode(user_sent, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "# 3) 장소별 combined_text 임베딩 (한 번만 수행)\n",
        "emb_corpus = model.encode(\n",
        "    df['combined_text'].tolist(),\n",
        "    convert_to_tensor=True\n",
        ")\n",
        "\n",
        "cos_scores = util.pytorch_cos_sim(emb_user, emb_corpus)[0]  # tensor of length N\n",
        "top_k = 10\n",
        "top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "# 6) 결과 출력\n",
        "top_indices = top_results.indices.cpu().numpy()\n",
        "top_scores  = top_results.values.cpu().numpy()\n",
        "recommendations = df.iloc[top_indices].copy()\n",
        "recommendations['score'] = top_scores\n",
        "\n",
        "# 최종 추천 리스트 보기\n",
        "print(recommendations[['contents_id','title','score']])"
      ],
      "metadata": {
        "id": "CBzHCtjU70lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Co-Review Data\n",
        "\n",
        "### 해야할 일 :\n",
        "- 관광지 + 음식점 파일로 구현해서 다시 돌리기\n",
        "\n",
        "\n",
        "Output Filename :\n",
        "/Sample/관광지_co_review_pairs.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "33ouVGXr7Wmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "# 1) 데이터 로드 및 컬럼 정리\n",
        "file_paths = glob('/content/drive/MyDrive/Sample_관광지/네이버 지도 방문자 리뷰 크롤러_관광지_*.xlsx')\n",
        "\n",
        "df = pd.concat([\n",
        "    pd.read_excel(\n",
        "        fp,\n",
        "        usecols=[\n",
        "            '가게이름','카테고리','전체평점',\n",
        "            '방문자리뷰','리뷰작성자','이런_점이_좋아요','방문시간'\n",
        "        ],\n",
        "        engine='openpyxl'     # openpyxl 엔진 사용\n",
        "    )\n",
        "    for fp in file_paths\n",
        "], ignore_index=True)\n",
        "\n",
        "# 방문시간을 문자열로 변환\n",
        "df['방문시간'] = df['방문시간'].astype(str)\n",
        "\n",
        "# 연도 포함 여부 판단: '^\\d{2}\\.\\d{1,2}\\.\\d{1,2}' 형태인 경우 → 연도 있음\n",
        "has_year = df['방문시간'].str.contains(r'^\\d{2}\\.\\d{1,2}\\.\\d{1,2}')\n",
        "\n",
        "# 연도 없는 데이터에는 기본 연도 '24' 추가 (또는 원하는 연도)\n",
        "df.loc[~has_year, '방문시간'] = '24.' + df.loc[~has_year, '방문시간']\n",
        "\n",
        "# 날짜 문자열만 추출\n",
        "df['date_str'] = df['방문시간'].str.extract(r'(\\d{2}\\.\\d{1,2}\\.\\d{1,2})')[0]\n",
        "\n",
        "# datetime 변환\n",
        "df['방문시간'] = pd.to_datetime(\n",
        "    df['date_str'],\n",
        "    format='%y.%m.%d',\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# 분기만 추출\n",
        "df['quarter'] = df['방문시간'].dt.quarter\n",
        "\n",
        "# 불필요한 중간 컬럼 제거\n",
        "df.drop(columns=['date_str'], inplace=True)\n",
        "\n",
        "#print(df[['가게이름','방문시간','quarter']].head(20))\n",
        "\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "# 1. 필요 컬럼만 추출\n",
        "df = df[['가게이름', '리뷰작성자', 'quarter']].dropna()\n",
        "\n",
        "# 2. 사용자-분기 기준 그룹핑하여 장소 목록 생성\n",
        "user_quarter_group = df.groupby(['리뷰작성자', 'quarter'])['가게이름'].apply(list)\n",
        "\n",
        "# 3. 각 그룹에서 장소 쌍 조합 생성\n",
        "co_review_pairs = []\n",
        "\n",
        "for places in user_quarter_group:\n",
        "    # 장소가 2개 이상인 경우만 조합\n",
        "    if len(set(places)) >= 2:\n",
        "        pairs = combinations(sorted(set(places)), 2)\n",
        "        co_review_pairs.extend(pairs)\n",
        "\n",
        "# 4. 장소 쌍별 등장 횟수 계산\n",
        "pair_counts = Counter(co_review_pairs)\n",
        "\n",
        "# 5. 결과를 DataFrame으로 변환\n",
        "pair_df = pd.DataFrame([\n",
        "    {'가게1': p1, '가게2': p2, 'co_review_count': count}\n",
        "    for (p1, p2), count in pair_counts.items()\n",
        "])\n",
        "\n",
        "# 6. co_review 수 내림차순 정렬\n",
        "pair_df = pair_df.sort_values(by='co_review_count', ascending=False)\n",
        "\n",
        "# 7. 결과 확인\n",
        "print(pair_df.head(20))\n",
        "\n",
        "# 8. 저장 (선택사항)\n",
        "pair_df.to_csv('/content/drive/MyDrive/Sample/관광지_co_review_pairs.csv', index=False)\n"
      ],
      "metadata": {
        "id": "auC2n3Yl7ccy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}